{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyaudi import gdual\n",
    "from pyaudi import sin, cos, tanh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent to train a neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the architecture of the network:\n",
    "\n",
    "    Inputs: 3\n",
    "    Hidden layers: 2 with 5 units/layer\n",
    "    Outputs: 1\n",
    "\n",
    "We will need the first order derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_units = [3, 5, 5, 1]\n",
    "order = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create symbolic variables for the weights with values drawn from $\\mathcal N(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n_units, order):\n",
    "\n",
    "    weights = []\n",
    "\n",
    "    for layer in range(1, len(n_units)):\n",
    "        weights.append([])\n",
    "        for unit in range(n_units[layer]):\n",
    "            weights[-1].append([])\n",
    "            for prev_unit in range(n_units[layer-1]):                \n",
    "                symname = 'w_{{({0},{1},{2})}}'.format(layer, unit, prev_unit)\n",
    "                w = gdual(np.random.randn(), symname , order)\n",
    "                weights[-1][-1].append(w)\n",
    "          \n",
    "    return weights\n",
    "\n",
    "weights = initialize_weights(n_units, order)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the biases, initialized to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_biases(n_units, order):\n",
    "\n",
    "    biases = []\n",
    "\n",
    "    for layer in range(1, len(n_units)):\n",
    "        biases.append([])\n",
    "        for unit in range(n_units[layer]):\n",
    "            symname = 'b_{{({0},{1})}}'.format(layer, unit)\n",
    "            b = gdual(1, symname , order)\n",
    "            biases[-1].append(b)\n",
    "            \n",
    "    return biases\n",
    "\n",
    "biases = initialize_biases(n_units, order)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network as a gdual expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function which output is the expression (*gdual*) corresponding to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def N_f(inputs, w, b):\n",
    "        \n",
    "    prev_layer_outputs = inputs\n",
    "    \n",
    "    #Hidden layers\n",
    "    for layer in range(len(weights)):\n",
    "        \n",
    "        this_layer_outputs = []\n",
    "        \n",
    "        for unit in range(len(weights[layer])):\n",
    "            \n",
    "            unit_output = 0\n",
    "            unit_output += b[layer][unit]\n",
    "            \n",
    "            for prev_unit,prev_output in enumerate(prev_layer_outputs):\n",
    "                unit_output += w[layer][unit][prev_unit] * prev_output\n",
    "            \n",
    "            if layer != len(weights)-1:\n",
    "                unit_output = tanh(unit_output)\n",
    "                \n",
    "            this_layer_outputs.append(unit_output)\n",
    "            \n",
    "        prev_layer_outputs = this_layer_outputs\n",
    "\n",
    "    return prev_layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new function can be used to compute the output of the network given any input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N(x) = 1.1707564339463399\n"
     ]
    }
   ],
   "source": [
    "x1 = 1\n",
    "x2 = 2\n",
    "x3 = 4\n",
    "\n",
    "N = N_f([x1,x2, x3], weights, biases)[0]\n",
    "print('N(x) = {0}'.format(N.constant_cf))\n",
    "#N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the desired output of the network will be: $y(\\mathcal x)= x_1x_2 + 0.5x_3 +2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 6.0\n"
     ]
    }
   ],
   "source": [
    "def y_f(x):\n",
    "    return x[0]*x[1] + 0.5*x[2] + 2\n",
    "\n",
    "y = y_f([x1,x2,x3])\n",
    "print('y = {0}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process will seek to minimize a loss function corresponding to the quadratic error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 23.32159342027067\n"
     ]
    }
   ],
   "source": [
    "def loss_f(N,y):\n",
    "    return (N-y)**2\n",
    "\n",
    "loss = loss_f(N, y)\n",
    "print('loss = {0}'.format(loss.constant_cf))\n",
    "#loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function loss_f accepts a gdual (N) and a float (y) and the output still a gdual, which will make it possible to compute its derivatives wrt any of the parameters of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_{(2,4,0)}\n",
      "w_{(2,4,1)}\n",
      "w_{(2,4,2)}\n",
      "w_{(2,4,3)}\n",
      "w_{(2,4,4)}\n",
      "w_{(3,0,0)}\n",
      "w_{(3,0,1)}\n",
      "w_{(3,0,2)}\n",
      "w_{(3,0,3)}\n",
      "w_{(3,0,4)}\n"
     ]
    }
   ],
   "source": [
    "for symbol in loss.symbol_set[-10:]:\n",
    "    print(symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training (Gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the weights with gradient descent using the first order derivatives of the loss function with respect to the weights (and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GD_update(loss, w, b, lr):\n",
    "    \n",
    "    #Update weights\n",
    "    for layer in range(len(w)):\n",
    "        for unit in range(len(w[layer])):\n",
    "            for prev_unit in range(len(w[layer][unit])):\n",
    "                \n",
    "                weight = w[layer][unit][prev_unit]\n",
    "                if weight.symbol_set[0] in loss.symbol_set:\n",
    "                    symbol_idx = loss.symbol_set.index(weight.symbol_set[0])\n",
    "                    d_idx = [0]*loss.symbol_set_size                                                      \n",
    "                    d_idx[symbol_idx] = 1\n",
    "                    \n",
    "                    # eg. if d_idx = [1,0,0,0,...] get get the derivatives of loss wrt\n",
    "                    # the first symbol (variable) in loss.symbol_set\n",
    "                    w[layer][unit][prev_unit] -= loss.get_derivative(d_idx) * lr\n",
    "\n",
    "    #Update biases\n",
    "    for i in range(len(b)):\n",
    "        for j in range(len(b[layer])):\n",
    "            \n",
    "                bias = b[layer][unit]\n",
    "                if bias.symbol_set[0] in loss.symbol_set:\n",
    "                    symbol_idx = loss.symbol_set.index(bias.symbol_set[0])\n",
    "                    d_idx = [0]*loss.symbol_set_size                    \n",
    "                    d_idx[symbol_idx] = 1\n",
    "                    b[layer][unit] -= loss.get_derivative(d_idx) * lr\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one GD step the loss function decrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 14.075570338997618\n"
     ]
    }
   ],
   "source": [
    "weights, biases = GD_update(loss, weights, biases, 0.01) \n",
    "\n",
    "N = N_f([x1,x2,x3], weights, biases)[0]\n",
    "loss = loss_f(N, y)\n",
    "print('loss = {0}'.format(loss.constant_cf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same for several data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = -1+2*np.random.rand(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49, training loss: 0.24293524790517088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7ff11bb51ba8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff11bb384a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = []\n",
    "\n",
    "weights = initialize_weights(n_units, order)                \n",
    "biases = initialize_biases(n_units, order)            \n",
    "\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for e in range(epochs):\n",
    "        \n",
    "    #Just records the error at the beginning of this step\n",
    "    epoch_loss = []\n",
    "    for xi in X:\n",
    "        N = N_f(xi, weights, biases)[0]\n",
    "        loss = loss_f(N, y_f(xi))\n",
    "        epoch_loss.append(loss.constant_cf)    \n",
    "        \n",
    "    loss_history.append(np.mean(epoch_loss))\n",
    "    \n",
    "    #Updates the weights\n",
    "    for xi in X:\n",
    "        N = N_f(xi, weights, biases)[0]\n",
    "        loss = loss_f(N, y_f(xi))\n",
    "        weights, biases = GD_update(loss, weights, biases, 0.001)     \n",
    "\n",
    "print('epoch {0}, training loss: {1}'.format(e, loss_history[-1]))        \n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
